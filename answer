#!/usr/bin/python

# answer
# 11-411 NLP Spring 2013, Group 6

# To run, type ./answer article_name
# e.g. ./answer

# Useful tools which should be pre-installed
import os, sys, errno
import subprocess
import re
import itertools
import nltk
from nltk.stem import PorterStemmer
import bs4
# Import our modules from /modules
sys.path.append("modules")
import questionClassifier
import sourceContentSelector

# To answer yes/no question, we want to just answer yes or no,
# and not returna  whole sentence. We do this by checking for
# any negatives in the sentence.
def contains_negative(sent):
  return "no" in sent or "not" in sent or "n't" in sent

# the set of pronouns, used for anaphora resolution
pronouns = set(["he", "she", "it", "him", "her", "his","they","their","we",
                "our","i","you","your","my","mine","yours","ours"])

# Runs coreference resolution on the article using arkref.
# This still needs to be implemented.
def coref(path_to_article):
  subprocess.call(["./arkref.sh", "-input", path_to_article])
  print open(path_to_article).read()
  tagged_article = open(path_to_article.replace("txt", "tagged")).read()
  tagged_article = "<root>"+tagged_article+"</root>" # trick arkref into doing entire doc
  #print tagged_article
  soup = bs4.BeautifulSoup(tagged_article, "html.parser").root
  for entity in soup.find_all(True):
    if entity.string != None and entity.string.strip().lower() in pronouns:
      antecedent_id = entity["entityid"].split("_")[0]
      antecedent = soup.find(mentionid=antecedent_id)
      string = re.sub('<.*?>',' ',str(antecedent))
      tok = nltk.word_tokenize(string)
      ants = [(x,y) for x,y in nltk.pos_tag(tok) if y in {'NNP','NN'}]
      entity.string.replace_with(' '.join(map(lambda (x,y):x,ants)))
      #print 'entity is: '+entity.string 
    #entity.unwrap()
  string2 = re.sub('<.*?>',' ',str(soup))
  print string2

  return open(path_to_article).read()

# Answers a question from the information in article.
# Ranks all the sentences and then returns the top choice.
def answer(question, article):
    question = question.strip()
    question_type = questionClassifier.process(question)
    question = nltk.tokenize.word_tokenize(question)
    relevant = sourceContentSelector.getScoredSentences(question, article)
    top = max(relevant, key = lambda s: s[1])
    if question_type == "BOOLEAN":
      if contains_negative(top): return "No"
      else: return "Yes"
    else:
      return top

# The main script
if __name__ == '__main__':
  article_name = sys.argv[1]

# for year in ("S08", "S09", "S10"):
#    print "Year:", year
#    prefix = "Question_Answer_Dataset_v1.1/"+year+"/"
#    question_answer_pairs = open(prefix+"question_answer_pairs.txt").readlines()
#    question_answer_pairs.pop(0)
#    for line in question_answer_pairs:
#      if not line.startswith(article_name): continue
#      line = line.lstrip(article_name)
#      end = line.find("?")
#      if end == -1: continue
#      question = line[:end+1].strip()
#      line = line[end+1:].split()
#      path_to_article = prefix+line.pop()+".txt"
#      difficulty_answerer = line.pop()
#      difficulty_questioner = line.pop()
#      correct_answer = " ".join(line)

    #print "Question:", question
    #print "Difficulty from answerer:", difficulty_answerer
    #print "Difficulty from questioner:", difficulty_questioner

      # Open the question file and start answering questions.
  article = coref('corefTest.txt')
  print article
    #print "Our answer:", answer(question, article)
    #print "Correct answer:", correct_answer
