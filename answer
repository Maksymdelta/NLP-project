#!/usr/bin/python

# answer
# 11-411 NLP Spring 2013, Group 6
# (Stub) Authored by Ryhan Hassan | rhassan@andrew.cmu.edu

# Useful tools which should be pre-installed
import os, sys, errno
import re
import itertools
import nltk

# Import our modules from /modules
sys.path.append("modules")

# Returns string containing content at file path.
def read_file(path):
  return open(path).read()

# basically just the parts of speecn any "important" word would be
key_POS = set(["CD","FW","NN","NNS","NP","NPS","VB","VBD","VBG","VBN","VBP","VBZ"])

def getKeywords(question):
    tagged = nltk.tag.pos_tag(question)
    return [pair for pair in tagged if pair[1] in key_POS]

def getRelevantSentences(keywords, article):
    relevant = []
    sentences = nltk.tokenize.sent_tokenize(article)
    for sent in sentences:
        sent = set(nltk.tokenize.word_tokenize(sent))
        good = True
        for word in keywords:
            if word not in sent:
                good = False
                break
        if good:
            relevant.append(sent)
    return relevant


def answer(question, article):
    question = question.strip()
    question = nltk.tokenize.word_tokenize(question)
    words = getKeywords(question)
    return getRelevantSentences(words, article)


if __name__ == '__main__':
  path_to_article = sys.argv[1]
  path_to_questions = sys.argv[2]

  # Pre-process article content.
  article_content = read_file(path_to_article)

  # Open the question file and start answering questions.
    with open(path_to_questions) as question_file:
        for line in question_file:
            print question
            print answer(question)

